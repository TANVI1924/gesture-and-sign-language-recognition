{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272158b8-e9c0-4fed-aca7-6b12ec2c6699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "from gtts import gTTS\n",
    "import pygame\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c73c60f-4d28-4801-abf0-5918ec854203",
   "metadata": {},
   "outputs": [],
   "source": [
    "GESTURE_CLASSES = ['Dislike', 'Fist', 'Hello', 'Like', 'Peace', 'Stop']\n",
    "ASL_CLASSES = [\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
    "    'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "    'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67a22ad2-43ad-4b15-a67e-54a17f269758",
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_model = load_model(\"mobilenetv2_gesture_model.keras\", compile=False)\n",
    "asl_model = load_model(\"asl_mobilenet_model.keras\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a3eec71-fe69-475e-84d1-ac0186c543f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_spoken = \"\"\n",
    "last_spoken_time = 0\n",
    "speak_cooldown = 2  \n",
    "prediction_buffer = []\n",
    "buffer_size = 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b646c4f-67d4-495d-a561-bcf729930f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_now(text):\n",
    "    try:\n",
    "        filename = f\"speech_{uuid.uuid4().hex}.mp3\" \n",
    "        tts = gTTS(text=text, lang='en')\n",
    "        tts.save(filename)\n",
    "\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(filename)\n",
    "        pygame.mixer.music.play()\n",
    "\n",
    "        while pygame.mixer.music.get_busy():\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        pygame.mixer.music.unload()\n",
    "        os.remove(filename)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[TTS] Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb198ce-6a16-4244-a0e3-38716153c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_camera():\n",
    "    global cap, running\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    running = True\n",
    "    show_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf51991f-5448-481d-a27a-208b90b2fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_camera():\n",
    "    global running\n",
    "    running = False\n",
    "    if cap:\n",
    "        cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516084c7-f6f6-424a-b328-20d9005765e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_frame():\n",
    "    global last_spoken, last_spoken_time\n",
    "\n",
    "    if not running:\n",
    "        return\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    roi = frame[100:400, 100:400]\n",
    "    cv2.rectangle(frame, (100, 100), (400, 400), (255, 0, 0), 2)\n",
    "\n",
    "    \n",
    "    roi_resized = cv2.resize(roi, (224, 224))\n",
    "    roi_input = preprocess_input(roi_resized.astype('float32'))\n",
    "    roi_input = np.expand_dims(roi_input, axis=0)\n",
    "\n",
    "    \n",
    "    gesture_pred = gesture_model.predict(roi_input, verbose=0)[0]\n",
    "    asl_pred = asl_model.predict(roi_input, verbose=0)[0]\n",
    "\n",
    "    gesture_label = GESTURE_CLASSES[np.argmax(gesture_pred)]\n",
    "    asl_label = ASL_CLASSES[np.argmax(asl_pred)]\n",
    "\n",
    "    gesture_conf = np.max(gesture_pred)\n",
    "    asl_conf = np.max(asl_pred)\n",
    "\n",
    "    final_gesture = gesture_label if gesture_conf > 0.7 else \"None\"\n",
    "    final_asl = asl_label if asl_conf > 0.7 else \"None\"\n",
    "\n",
    "    \n",
    "    gesture_var.set(f\"Gesture: {final_gesture}\")\n",
    "    asl_var.set(f\"ASL: {final_asl}\")\n",
    "\n",
    "    \n",
    "    prediction_buffer.append((final_asl, final_gesture, asl_conf, gesture_conf))\n",
    "    if len(prediction_buffer) > buffer_size:\n",
    "        prediction_buffer.pop(0)\n",
    "\n",
    "    \n",
    "    asl_labels = [p[0] for p in prediction_buffer]\n",
    "    gesture_labels = [p[1] for p in prediction_buffer]\n",
    "    asl_confs = [p[2] for p in prediction_buffer]\n",
    "    gesture_confs = [p[3] for p in prediction_buffer]\n",
    "\n",
    "    stable_asl = asl_labels.count(asl_labels[-1]) == buffer_size and asl_labels[-1] not in ['nothing', 'None']\n",
    "    stable_gesture = gesture_labels.count(gesture_labels[-1]) == buffer_size and gesture_labels[-1] != \"None\"\n",
    "\n",
    "    avg_asl_conf = sum(asl_confs) / len(asl_confs)\n",
    "    avg_gesture_conf = sum(gesture_confs) / len(gesture_confs)\n",
    "\n",
    "    current_time = time.time()\n",
    "\n",
    "        \n",
    "    stable_asl = asl_labels.count(asl_labels[-1]) == buffer_size and asl_labels[-1] not in ['nothing', 'None']\n",
    "    stable_gesture = gesture_labels.count(gesture_labels[-1]) == buffer_size and gesture_labels[-1] != \"None\"\n",
    "\n",
    "    avg_asl_conf = sum(asl_confs) / len(asl_confs)\n",
    "    avg_gesture_conf = sum(gesture_confs) / len(gesture_confs)\n",
    "\n",
    "    current_time = time.time()\n",
    "       \n",
    "    predicted_text = None\n",
    "\n",
    "    similar_pairs = {\n",
    "        \"Peace\": \"V\",\n",
    "        \"Fist\": \"S\"\n",
    "    }\n",
    "\n",
    "    gesture = gesture_labels[-1]\n",
    "    asl = asl_labels[-1]\n",
    "\n",
    "    is_similar_pair = (\n",
    "        (gesture in similar_pairs and similar_pairs[gesture] == asl) or\n",
    "        (asl in similar_pairs.values() and any(k for k, v in similar_pairs.items() if v == asl and k == gesture))\n",
    "    )\n",
    "\n",
    "    if stable_asl and stable_gesture and is_similar_pair and avg_asl_conf > 0.75 and avg_gesture_conf > 0.75:\n",
    "        predicted_text = f\"{gesture} and letter {asl}\"\n",
    "\n",
    "    elif stable_gesture and avg_gesture_conf > 0.75:\n",
    "        predicted_text = gesture\n",
    "    elif stable_asl and avg_asl_conf > 0.75:\n",
    "        predicted_text = asl\n",
    "\n",
    "    \n",
    "    \n",
    "    if predicted_text and (predicted_text != last_spoken or (current_time - last_spoken_time) > 5):\n",
    "        last_spoken = predicted_text\n",
    "        last_spoken_time = current_time\n",
    "        threading.Thread(target=speak_now, args=(predicted_text,), daemon=True).start()\n",
    "        \n",
    "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    video_label.imgtk = imgtk   \n",
    "    video_label.configure(image=imgtk)  \n",
    "\n",
    "    video_label.after(10, show_frame)   \n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "437c3d1e-7c82-463e-bd5a-ea88c0628100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"ASL + Gesture Recognition\")\n",
    "\n",
    "video_label = tk.Label(root)\n",
    "video_label.pack()\n",
    "\n",
    "gesture_var = tk.StringVar()\n",
    "asl_var = tk.StringVar()\n",
    "\n",
    "tk.Label(root, textvariable=gesture_var, font=(\"Helvetica\", 14)).pack()\n",
    "tk.Label(root, textvariable=asl_var, font=(\"Helvetica\", 14)).pack()\n",
    "\n",
    "\n",
    "confidence_var = tk.StringVar()\n",
    "tk.Label(root, textvariable=confidence_var, font=(\"Helvetica\", 10)).pack()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6c7fd98-36ad-4b02-a5d4-051da57b9d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_confidence():\n",
    "    if prediction_buffer:\n",
    "        asl_confs = [p[2] for p in prediction_buffer]\n",
    "        gesture_confs = [p[3] for p in prediction_buffer]\n",
    "        avg_asl_conf = sum(asl_confs) / len(asl_confs)\n",
    "        avg_gesture_conf = sum(gesture_confs) / len(gesture_confs)\n",
    "        confidence_var.set(f\"ASL Conf: {avg_asl_conf:.2f} | Gesture Conf: {avg_gesture_conf:.2f}\")\n",
    "    root.after(500, update_confidence)\n",
    "\n",
    "update_confidence()\n",
    "\n",
    "\n",
    "button_frame = tk.Frame(root)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "tk.Button(button_frame, text=\"Start Camera\", command=start_camera).grid(row=0, column=0, padx=5)\n",
    "tk.Button(button_frame, text=\"Stop Camera\", command=stop_camera).grid(row=0, column=1, padx=5)\n",
    "\n",
    "root.protocol(\"WM_DELETE_WINDOW\", lambda: (stop_camera(), root.destroy()))\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb30950-c829-4825-8924-ee19c1fd2905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TF)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
